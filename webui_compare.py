#!/usr/bin/env python3
"""
IndexTTS WebUI with Model Comparison
ì›ë³¸ ëª¨ë¸ vs í•œêµ­ì–´ ëª¨ë¸ ë¹„êµ ê¸°ëŠ¥ ì¶”ê°€
"""

import html
import json
import os
import sys
import threading
import time
import warnings

warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)

import pandas as pd

current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.append(current_dir)
sys.path.append(os.path.join(current_dir, "indextts"))

import argparse
parser = argparse.ArgumentParser(
    description="IndexTTS WebUI with Model Comparison",
    formatter_class=argparse.ArgumentDefaultsHelpFormatter,
)
parser.add_argument("--verbose", action="store_true", default=False, help="Enable verbose mode")
parser.add_argument("--port", type=int, default=7860, help="Port to run the web UI on")
parser.add_argument("--host", type=str, default="0.0.0.0", help="Host to run the web UI on")
parser.add_argument("--model_dir", type=str, default="./checkpoints", help="Model checkpoints directory")
parser.add_argument("--fp16", action="store_true", default=False, help="Use FP16 for inference if available")
parser.add_argument("--deepspeed", action="store_true", default=False, help="Use DeepSpeed to accelerate if available")
parser.add_argument("--cuda_kernel", action="store_true", default=False, help="Use CUDA kernel for inference if available")
parser.add_argument("--gui_seg_tokens", type=int, default=120, help="GUI: Max tokens per generation segment")
cmd_args = parser.parse_args()

import gradio as gr
from indextts.infer_v2 import IndexTTS2
from tools.i18n.i18n import I18nAuto

i18n = I18nAuto(language="Auto")
MODE = 'local'

# ëª¨ë¸ ì„¤ì •
MODEL_CONFIGS = {
    "ì›ë³¸ (Original - 12K tokens)": {
        "config": "checkpoints/config_original.yaml.backup",
        "gpt_checkpoint": "/mnt/sda1/models/IndexTTS-2/gpt.pth",
        "tokenizer": "/mnt/sda1/models/IndexTTS-2/bpe.model",
        "description": "ì¤‘êµ­ì–´+ì˜ì–´ ì›ë³¸ ëª¨ë¸ (12000 tokens)"
    },
    "í•œêµ­ì–´ (Korean - 16K tokens)": {
        "config": "checkpoints/config_korean.yaml",
        "gpt_checkpoint": "/mnt/sda1/models/index-tts-ko/checkpoints/latest.pth",
        "tokenizer": "/mnt/sda1/models/IndexTTS-2/tokenizer_ko/ko_bpe.model",
        "description": "í•œêµ­ì–´ í•™ìŠµ ëª¨ë¸ (16000 tokens, step 297000)"
    }
}

# í˜„ì¬ ë¡œë“œëœ ëª¨ë¸
current_model_name = None
tts = None

def load_model(model_name):
    global current_model_name, tts

    if current_model_name == model_name and tts is not None:
        return f"âœ“ ì´ë¯¸ '{model_name}' ëª¨ë¸ì´ ë¡œë“œë˜ì–´ ìˆìŠµë‹ˆë‹¤."

    model_config = MODEL_CONFIGS[model_name]

    print(f"\n{'='*60}")
    print(f"ëª¨ë¸ ì „í™˜ ì¤‘: {model_name}")
    print(f"  Config: {model_config['config']}")
    print(f"  GPT: {model_config['gpt_checkpoint']}")
    print(f"  Tokenizer: {model_config['tokenizer']}")
    print(f"{'='*60}\n")

    # ê¸°ì¡´ ëª¨ë¸ í•´ì œ
    if tts is not None:
        del tts
        import torch
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

    # ìƒˆ ëª¨ë¸ ë¡œë“œ
    tts = IndexTTS2(
        model_dir=cmd_args.model_dir,
        cfg_path=model_config['config'],
        use_fp16=cmd_args.fp16,
        use_deepspeed=cmd_args.deepspeed,
        use_cuda_kernel=cmd_args.cuda_kernel,
    )

    current_model_name = model_name

    return f"âœ“ '{model_name}' ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!\n{model_config['description']}"

# ì´ˆê¸° ëª¨ë¸ ë¡œë“œ
load_model("ì›ë³¸ (Original - 12K tokens)")

EMO_CHOICES_ALL = [i18n("ä¸éŸ³è‰²å‚è€ƒéŸ³é¢‘ç›¸åŒ"),
                i18n("ä½¿ç”¨æƒ…æ„Ÿå‚è€ƒéŸ³é¢‘"),
                i18n("ä½¿ç”¨æƒ…æ„Ÿå‘é‡æ§åˆ¶"),
                i18n("ä½¿ç”¨æƒ…æ„Ÿæè¿°æ–‡æœ¬æ§åˆ¶")]
EMO_CHOICES_OFFICIAL = EMO_CHOICES_ALL[:-1]

os.makedirs("outputs/tasks", exist_ok=True)
os.makedirs("prompts", exist_ok=True)

def gen_single(prompt, text, emo_control_method, emo_ref, emo_weight, emo_text,
               vec1, vec2, vec3, vec4, vec5, vec6, vec7, vec8,
               emo_random, max_text_tokens_per_segment, model_name, **kwargs):

    # ì„ íƒëœ ëª¨ë¸ë¡œ ì „í™˜
    load_model(model_name)

    if prompt is None:
        return gr.update(value=None, visible=True)

    output_path = f"outputs/spk_{int(time.time())}.wav"

    emo_ref_path = emo_ref
    if emo_control_method == 0:
        emo_ref_path = None
    if emo_control_method == 1:
        pass
    if emo_control_method == 2:
        vec = [vec1, vec2, vec3, vec4, vec5, vec6, vec7, vec8]
        vec = tts.normalize_emo_vec(vec, apply_bias=True)
    else:
        vec = None

    if emo_text == "":
        emo_text = None

    print(f"[{model_name}] Emo control mode:{emo_control_method},weight:{emo_weight},vec:{vec}")
    output = tts.infer(
        spk_audio_prompt=prompt,
        text=text,
        output_path=output_path,
        emo_audio_prompt=emo_ref_path,
        emo_alpha=emo_weight,
        emo_vector=vec,
        use_emo_text=(emo_control_method==3),
        emo_text=emo_text,
        use_random=emo_random,
        verbose=cmd_args.verbose,
        max_text_tokens_per_segment=int(max_text_tokens_per_segment),
        **kwargs
    )
    return gr.update(value=output, visible=True)

with gr.Blocks(title="IndexTTS Model Comparison") as demo:
    mutex = threading.Lock()
    gr.HTML('''
    <h2><center>IndexTTS2 Model Comparison: ì›ë³¸ vs í•œêµ­ì–´ ëª¨ë¸</center></h2>
    <p align="center">ì›ë³¸ ëª¨ë¸ê³¼ í•œêµ­ì–´ í•™ìŠµ ëª¨ë¸ì„ ë¹„êµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤</p>
    ''')

    with gr.Tab("ìŒì„± ìƒì„± ë° ëª¨ë¸ ë¹„êµ"):
        # ëª¨ë¸ ì„ íƒ
        with gr.Row():
            model_selector = gr.Radio(
                choices=list(MODEL_CONFIGS.keys()),
                value="ì›ë³¸ (Original - 12K tokens)",
                label="ğŸ”„ ëª¨ë¸ ì„ íƒ",
                info="ëª¨ë¸ì„ ì„ íƒí•˜ë©´ ìë™ìœ¼ë¡œ ì „í™˜ë©ë‹ˆë‹¤"
            )
            model_status = gr.Textbox(
                label="ëª¨ë¸ ìƒíƒœ",
                value="âœ“ ì›ë³¸ ëª¨ë¸ ë¡œë“œë¨",
                interactive=False,
                lines=2
            )

        with gr.Row():
            prompt_audio = gr.Audio(
                label="ìŒìƒ‰ ì°¸ì¡° ì˜¤ë””ì˜¤",
                sources=["upload", "microphone"],
                type="filepath"
            )
            with gr.Column():
                input_text_single = gr.TextArea(
                    label="í…ìŠ¤íŠ¸",
                    placeholder="ìƒì„±í•  í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”",
                    info="ì›ë³¸: ì¤‘êµ­ì–´/ì˜ì–´, í•œêµ­ì–´: í•œêµ­ì–´"
                )
                gen_button = gr.Button("ğŸ™ï¸ ìƒì„±", variant="primary")
            output_audio = gr.Audio(label="ìƒì„± ê²°ê³¼", visible=True)

        with gr.Accordion("ê³ ê¸‰ ì„¤ì •", open=False):
            with gr.Row():
                emo_control_method = gr.Radio(
                    choices=EMO_CHOICES_OFFICIAL,
                    type="index",
                    value=EMO_CHOICES_OFFICIAL[0],
                    label="ê°ì • ì œì–´ ë°©ì‹"
                )

            with gr.Group(visible=False) as emotion_reference_group:
                emo_upload = gr.Audio(label="ê°ì • ì°¸ì¡° ì˜¤ë””ì˜¤", type="filepath")

            with gr.Row(visible=False) as emotion_randomize_group:
                emo_random = gr.Checkbox(label="ê°ì • ëœë¤ ìƒ˜í”Œë§", value=False)

            with gr.Group(visible=False) as emotion_vector_group:
                gr.Markdown("**ê°ì • ë²¡í„° ì¡°ì •** (0.0 ~ 1.0)")
                with gr.Row():
                    vec1 = gr.Slider(0, 1, 0, step=0.05, label="Happy")
                    vec2 = gr.Slider(0, 1, 0, step=0.05, label="Angry")
                    vec3 = gr.Slider(0, 1, 0, step=0.05, label="Sad")
                    vec4 = gr.Slider(0, 1, 0, step=0.05, label="Afraid")
                with gr.Row():
                    vec5 = gr.Slider(0, 1, 0, step=0.05, label="Disgusted")
                    vec6 = gr.Slider(0, 1, 0, step=0.05, label="Melancholic")
                    vec7 = gr.Slider(0, 1, 0, step=0.05, label="Surprised")
                    vec8 = gr.Slider(0, 1, 0, step=0.05, label="Calm")

            emo_weight = gr.Slider(0, 1, 1.0, step=0.05, label="ê°ì • ê°•ë„")
            emo_text = gr.Textbox(label="ê°ì • í…ìŠ¤íŠ¸ ì„¤ëª…", value="", visible=False)
            max_text_tokens_per_segment = gr.Number(value=120, label="ì„¸ê·¸ë¨¼íŠ¸ë‹¹ ìµœëŒ€ í† í°")

        # ëª¨ë¸ ì „í™˜ ì´ë²¤íŠ¸
        model_selector.change(
            fn=load_model,
            inputs=[model_selector],
            outputs=[model_status]
        )

        # ê°ì • ì œì–´ UI ì—…ë°ì´íŠ¸
        def update_emotion_ui(method):
            return {
                emotion_reference_group: gr.update(visible=(method == 1)),
                emotion_vector_group: gr.update(visible=(method == 2)),
                emotion_randomize_group: gr.update(visible=(method in [2, 3])),
                emo_text: gr.update(visible=(method == 3))
            }

        emo_control_method.change(
            fn=update_emotion_ui,
            inputs=[emo_control_method],
            outputs=[emotion_reference_group, emotion_vector_group,
                    emotion_randomize_group, emo_text]
        )

        # ìƒì„± ë²„íŠ¼
        gen_button.click(
            fn=gen_single,
            inputs=[
                prompt_audio, input_text_single, emo_control_method,
                emo_upload, emo_weight, emo_text,
                vec1, vec2, vec3, vec4, vec5, vec6, vec7, vec8,
                emo_random, max_text_tokens_per_segment, model_selector
            ],
            outputs=[output_audio]
        )

    with gr.Tab("ğŸ“– ì‚¬ìš© ê°€ì´ë“œ"):
        gr.Markdown("""
        ## ëª¨ë¸ ë¹„êµ ê°€ì´ë“œ

        ### ì›ë³¸ ëª¨ë¸ (Original - 12K tokens)
        - **ì–¸ì–´**: ì¤‘êµ­ì–´, ì˜ì–´
        - **í† í°**: 12,000ê°œ
        - **ê°ì • ì œì–´**: âœ… ë§¤ìš° ìš°ìˆ˜ (ì¤‘êµ­ì–´/ì˜ì–´ ê°ì • ë°ì´í„° í•™ìŠµ)
        - **ì¶”ì²œ í…ìŠ¤íŠ¸**: ì˜ì–´ ë˜ëŠ” ì¤‘êµ­ì–´

        ### í•œêµ­ì–´ ëª¨ë¸ (Korean - 16K tokens)
        - **ì–¸ì–´**: í•œêµ­ì–´
        - **í† í°**: 16,000ê°œ (í•œêµ­ì–´ ì „ìš©)
        - **í•™ìŠµ**: Step 297,000ê¹Œì§€ fine-tuning
        - **ê°ì • ì œì–´**: âš ï¸ ì œí•œì  (ì›ë³¸ ê°ì • ë§¤íŠ¸ë¦­ìŠ¤ ì‚¬ìš©)
        - **ì¶”ì²œ í…ìŠ¤íŠ¸**: í•œêµ­ì–´ë§Œ

        ### ë¹„êµ í…ŒìŠ¤íŠ¸ ë°©ë²•

        1. **ê°™ì€ ìŒìƒ‰ ì°¸ì¡° ì˜¤ë””ì˜¤ ì‚¬ìš©**
        2. **ë‘ ëª¨ë¸ë¡œ ê°™ì€ ì˜ë¯¸ì˜ í…ìŠ¤íŠ¸ ìƒì„±**
           - ì›ë³¸: "Hello, this is a test."
           - í•œêµ­ì–´: "ì•ˆë…•í•˜ì„¸ìš”, ì´ê²ƒì€ í…ŒìŠ¤íŠ¸ì…ë‹ˆë‹¤."
        3. **ê°™ì€ ê°ì • ë²¡í„° ì„¤ì •** (ì˜ˆ: Sad = 0.8)
        4. **ê²°ê³¼ ë¹„êµ**

        ### ê°ì • ë²¡í„° ì¶”ì²œ ê°’
        - **Happy**: 0.6~0.8
        - **Sad**: 0.6~0.8
        - **Calm**: 0.5~0.7
        - **Surprised**: 0.7~0.9
        """)

if __name__ == "__main__":
    demo.queue().launch(
        server_name=cmd_args.host,
        server_port=cmd_args.port,
        share=False
    )
