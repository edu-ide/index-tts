#!/usr/bin/env bash
# Base ëª¨ë¸ë¶€í„° ìž¬í•™ìŠµ (Best Checkpoint ì €ìž¥ ê¸°ëŠ¥ ì¶”ê°€)

set -euo pipefail

echo "================================================================"
echo "Base ëª¨ë¸ë¶€í„° ìž¬í•™ìŠµ (Best Checkpoint ë³´ì¡´)"
echo "================================================================"
echo ""
echo "ê°œì„  ì‚¬í•­:"
echo "  âœ… Best checkpoint ìžë™ ì €ìž¥"
echo "  âœ… ìµœê·¼ 5ê°œ ì²´í¬í¬ì¸íŠ¸ ìœ ì§€ (ê¸°ì¡´ 3ê°œ â†’ 5ê°œ)"
echo "  âœ… Validation loss ê¸°ë¡ (VAL_INTERVAL=1000)"
echo ""
echo "================================================================"

# í™˜ê²½ í™•ì¸
if [[ -z "${VIRTUAL_ENV:-}" ]]; then
  echo "[ERROR] ê°€ìƒí™˜ê²½ì´ í™œì„±í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤." >&2
  echo "ì‹¤í–‰: source /mnt/sdc1/ws/workspace/.venv_indextts/bin/activate" >&2
  exit 1
fi

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
BASE_CHECKPOINT="/mnt/sda1/models/IndexTTS-2/gpt.pth"
BACKUP_DIR="/mnt/sda1/models/index-tts-ko/checkpoints_backup_$(date +%Y%m%d_%H%M%S)"

if [[ ! -f "${BASE_CHECKPOINT}" ]]; then
  echo "[ERROR] Base ì²´í¬í¬ì¸íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: ${BASE_CHECKPOINT}" >&2
  exit 1
fi

# ê¸°ì¡´ ì²´í¬í¬ì¸íŠ¸ ë°±ì—… í™•ì¸
echo "âš ï¸  ê²½ê³ : ê¸°ì¡´ í•™ìŠµ ë°ì´í„°ë¥¼ ë°±ì—…í•©ë‹ˆë‹¤."
echo "ë°±ì—… ìœ„ì¹˜: ${BACKUP_DIR}"
echo ""
read -p "ê³„ì†í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): " -n 1 -r
echo
if [[ ! $REPLY =~ ^[Yy]$ ]]; then
  echo "ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤."
  exit 0
fi

# ë°±ì—…
echo "ê¸°ì¡´ ì²´í¬í¬ì¸íŠ¸ ë°±ì—… ì¤‘..."
mkdir -p "${BACKUP_DIR}"
cp -r /mnt/sda1/models/index-tts-ko/checkpoints/* "${BACKUP_DIR}/" || true
echo "ë°±ì—… ì™„ë£Œ: ${BACKUP_DIR}"
echo ""

# Best checkpoint ëª¨ë‹ˆí„°ë§ ìŠ¤í¬ë¦½íŠ¸ ìƒì„± (ì²´í¬í¬ì¸íŠ¸ì˜ lossë¥¼ ì§ì ‘ ë¹„êµ)
cat > /tmp/monitor_best_checkpoint.py << 'EOF'
#!/usr/bin/env python3
"""
Best checkpoint ëª¨ë‹ˆí„°ë§ ë° ìžë™ ì €ìž¥ (ì²´í¬í¬ì¸íŠ¸ì— ì €ìž¥ëœ loss ê¸°ì¤€)
- latest.pthë¥¼ ì½ì–´ loss ë¹„êµ í›„ best_model_stepXXXX.pth ê°±ì‹ 
"""
import time
import shutil
from pathlib import Path

import torch

ckpt_dir = Path("/mnt/sda1/models/index-tts-ko/checkpoints")
latest_ckpt = ckpt_dir / "latest.pth"
best_loss_file = ckpt_dir / "best_loss.txt"
best_step_file = ckpt_dir / "best_step.txt"

if best_loss_file.exists():
    try:
        with open(best_loss_file, 'r') as f:
            best_loss = float(f.read().strip())
    except Exception:
        best_loss = float('inf')
else:
    best_loss = float('inf')

print("Best checkpoint monitor started (checkpoint loss criterion)")
print(f"  Current best loss: {best_loss if best_loss < float('inf') else 'inf'}")


def extract_loss(ckpt: dict):
    # validation ìš°ì„ , ì—†ìœ¼ë©´ None ë°˜í™˜
    extra = ckpt.get("extra") or {}
    candidates = [
        ("val_text_loss", ckpt.get("val_text_loss")),
        ("val_mel_loss", ckpt.get("val_mel_loss")),
        ("val_text_loss", extra.get("val_text_loss")),
        ("val_mel_loss", extra.get("val_mel_loss")),
    ]
    for name, value in candidates:
        if value is not None:
            try:
                return name, float(value)
            except Exception:
                continue
    return None, None


last_mtime = 0.0

while True:
    try:
        if not latest_ckpt.exists():
            time.sleep(30)
            continue

        mtime = latest_ckpt.stat().st_mtime
        if mtime == last_mtime:
            time.sleep(30)
            continue

        last_mtime = mtime

        try:
            ckpt = torch.load(latest_ckpt, map_location="cpu")
        except Exception as load_err:
            print(f"Error loading {latest_ckpt.name}: {load_err}")
            time.sleep(30)
            continue

        metric_name, current_loss = extract_loss(ckpt)
        if metric_name is None:
            print("Warning: no validation loss found in checkpoint; waiting for next validation")
            time.sleep(30)
            continue

        step = ckpt.get("step") or ckpt.get("global_step") or ckpt.get("epoch")
        if step is None:
            step = 0

        if current_loss < best_loss:
            best_loss = current_loss

            # ë³µì‚¬ ëŒ€ìƒ: latest.pth (rounding í•„ìš” ì—†ìœ¼ë¯€ë¡œ ê·¸ëŒ€ë¡œ ì‚¬ìš©)
            target_ckpt = latest_ckpt
            target_step = step

            for old_best in ckpt_dir.glob("best_model_step*.pth"):
                try:
                    old_best.unlink()
                except Exception:
                    pass
            legacy = ckpt_dir / "best_model.pth"
            if legacy.exists():
                try:
                    legacy.unlink()
                except Exception:
                    pass

            best_target = ckpt_dir / f"best_model_step{target_step}.pth"

            def _copy_best():
                tmp = best_target.with_suffix(best_target.suffix + ".tmp")
                shutil.copy2(target_ckpt, tmp)
                tmp.replace(best_target)

            import threading
            t = threading.Thread(target=_copy_best, daemon=True)
            t.start()
            t.join()
            with open(best_loss_file, 'w') as f:
                f.write(f"{best_loss:.6f}")
            with open(best_step_file, 'w') as f:
                f.write(str(step))

            print(
                f"\nðŸŽ¯ New best! step={step} metric={metric_name} loss={current_loss:.4f} "
                f"(copied from {target_ckpt.name} -> {best_target.name})\n"
            )

        time.sleep(30)

    except KeyboardInterrupt:
        print("\nMonitor stopped")
        break
    except Exception as e:
        print(f"Error: {e}")
        time.sleep(30)
EOF

# ë°±ê·¸ë¼ìš´ë“œë¡œ best checkpoint ëª¨ë‹ˆí„° ì‹¤í–‰
nohup python3 /tmp/monitor_best_checkpoint.py > /tmp/best_ckpt_monitor.log 2>&1 &
MONITOR_PID=$!
echo "Best checkpoint monitor started (PID: ${MONITOR_PID})"
echo "ë¡œê·¸: /tmp/best_ckpt_monitor.log"
echo ""

echo "í•™ìŠµì„ ì‹œìž‘í•©ë‹ˆë‹¤..."
echo "TensorBoard: http://localhost:6006"
echo ""
echo "ì°¸ê³ : ë°ì´í„° ê²€ì¦ ìŠ¤í‚µ (SKIP_DATA_CHECK=1)"
echo "      ê²€ì¦í•˜ë ¤ë©´: SKIP_DATA_CHECK=0 ./tools/restart_from_base_with_best.sh"
echo ""

# ìž¬í•™ìŠµ ì‹œìž‘
SKIP_DATA_CHECK=1 \
LR=5e-6 \
WARMUP_STEPS=10000 \
MAX_STEPS=0 \
GRAD_CLIP=0.5 \
BATCH_SIZE=8 \
LOG_INTERVAL=100 \
VAL_INTERVAL=1000 \
EPOCHS=2 \
BASE_CHECKPOINT="${BASE_CHECKPOINT}" \
"${SCRIPT_DIR}/ko_step4_train_gpt.sh"

# í•™ìŠµ ì™„ë£Œ í›„ ëª¨ë‹ˆí„° ì¢…ë£Œ
kill ${MONITOR_PID} 2>/dev/null || true

echo ""
echo "================================================================"
echo "ìž¬í•™ìŠµ ì™„ë£Œ!"
echo "================================================================"
echo ""
echo "ì €ìž¥ëœ ì²´í¬í¬ì¸íŠ¸:"
echo "  - ìµœê³  ì„±ëŠ¥: /mnt/sda1/models/index-tts-ko/checkpoints/best_model.pth"
echo "  - ìµœì‹ : /mnt/sda1/models/index-tts-ko/checkpoints/latest.pth"
echo "  - ìµœê·¼ 5ê°œ: model_step*.pth"
echo ""
cat /mnt/sda1/models/index-tts-ko/checkpoints/best_loss.txt 2>/dev/null && \
  echo "Best mel_loss: $(cat /mnt/sda1/models/index-tts-ko/checkpoints/best_loss.txt)"
