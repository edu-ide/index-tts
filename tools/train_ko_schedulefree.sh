#!/usr/bin/env bash
# Schedule-Free AdamW - Step 0ë¶€í„° ì™„ì „ ìƒˆë¡œ ì‹œì‘
set -euo pipefail

# Activate virtual environment
source /mnt/sdc1/ws/workspace/.venv_indextts/bin/activate

# Memory optimization
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True

echo "================================================================"
echo "ğŸš€ Schedule-Free AdamW ì™„ì „ ìƒˆë¡œ ì‹œì‘"
echo "================================================================"
echo ""
echo "ğŸ“Š ì‹¤í—˜ ì„¤ì •:"
echo "  - Optimizer: Schedule-Free AdamW (3-4x faster than Prodigy)"
echo "  - Learning Rate: 5e-4 (ê¶Œì¥ê°’)"
echo "  - No LR Scheduler needed (built-in warmup)"
echo "  - Starting from: Step 0 (fresh start)"
echo "  - ëª©í‘œ: Step 240kê¹Œì§€ ë¹ ë¥´ê³  ì•ˆì •ì ì¸ í•™ìŠµ"
echo ""

SCRIPT_DIR="/mnt/sdc1/ws/workspace/monorepo/external/index-tts"

cd "${SCRIPT_DIR}"

SKIP_DATA_CHECK=1 \
OPTIMIZER=schedulefree \
LR=5e-4 \
BATCH_SIZE=8 \
GRAD_ACC=1 \
LOG_INTERVAL=100 \
VAL_INTERVAL=10000 \
MAX_STEPS=240000 \
EPOCHS=999 \
NUM_WORKERS=16 \
"${SCRIPT_DIR}/tools/ko_step4_train_gpt.sh"
