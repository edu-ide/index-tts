#!/usr/bin/env bash
# Prodigy Optimizer ì¬ê°œ - ê¸°ì¡´ Prodigy checkpointì—ì„œ ì´ì–´ì„œ í•™ìŠµ
set -euo pipefail

# Activate virtual environment
source /mnt/sdc1/ws/workspace/.venv_indextts/bin/activate

echo "================================================================"
echo "ğŸ”„ Prodigy Optimizer í•™ìŠµ ì¬ê°œ"
echo "================================================================"
echo ""
echo "ğŸ“Š ì‹¤í—˜ ì„¤ì •:"
echo "  - Optimizer: Prodigy (parameter-free, auto LR)"
echo "  - Resume from: latest.pth (Prodigy checkpoint)"
echo "  - Optimizer state ìœ ì§€í•˜ì—¬ ì¬ê°œ"
echo ""

SCRIPT_DIR="/mnt/sdc1/ws/workspace/monorepo/external/index-tts"

cd "${SCRIPT_DIR}"

# Check if latest.pth exists
CKPT_PATH="${CKPT_PATH:-/mnt/sda1/models/index-tts-ko/checkpoints/latest.pth}"
if [[ ! -f "${CKPT_PATH}" ]]; then
    echo "âŒ Error: ${CKPT_PATH} not found!"
    echo "   Use train_ko_prodigy.sh to start fresh training first."
    exit 1
fi

SKIP_DATA_CHECK=1 \
OPTIMIZER=prodigy \
BATCH_SIZE=8 \
GRAD_ACC=1 \
AMP=1 \
LOG_INTERVAL=200 \
VAL_INTERVAL=10000 \
MAX_STEPS=240000 \
EPOCHS=999 \
NUM_WORKERS=12 \
RESUME="${CKPT_PATH}" \
"${SCRIPT_DIR}/tools/ko_step4_train_gpt.sh" --no-aim --scheduler none
