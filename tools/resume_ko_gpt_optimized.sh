#!/usr/bin/env bash
# μµμ ν™”λ ν•κµ­μ–΄ GPT μ¬κ° μ¤ν¬λ¦½νΈ
#
# μµμ ν™” μ„¤μ • (λ…Όλ¬Έ κΈ°λ°):
# - LR: 1e-5 (BERT/GPT fine-tuning κ¶μ¥ λ²”μ„ 1e-5~5e-5 λ‚΄)
# - Batch: 8 (RTX 4090 24GB κ²€μ¦λ μ•μ „κ°’)
# - Grad Accumulation: 8 (μ‹¤ν¨ batch 64 = ~34,694 tokens)
# - Warmup: 30,000 steps (ν„μ¬ ν•™μµ μ μ§€, λ‹¤μμ€ 5,000 κ¶μ¥)
# - Grad Clip: 0.5 (gradient explosion λ°©μ§€)
#
# μ°Έκ³ λ¬Έν—:
# [1] Attention is All You Need (Vaswani+ 2017) - ~25k tokens/batch
# [2] BERT (Devlin+ 2019) - fine-tuning LR
# [3] Decoupled Weight Decay (Loshchilov+ 2019) - AdamW
#
# Batch 8 Γ— Grad Acc 8 = 64 ν¨κ³Ό:
# - Total tokens/batch: ~34,694 (Transformer λ…Όλ¬Έ μμ¤€ β…)
# - RTX 4090 24GB λ©”λ¨λ¦¬ μ•μ „μ„± λ³΄μ¥ β…
# - mel_loss variance λ€ν­ κ°μ† μμƒ (~70%)
# - Training loss μ•μ •ν™”
# - ν•™μµ μ†λ„ μ•½κ°„ κ°μ† (ν—μ© λ²”μ„)

set -euo pipefail

echo "================================================================"
echo "π”„ μµμ ν™”λ ν•κµ­μ–΄ GPT μ¬κ° - Transformer λ…Όλ¬Έ κΈ°λ°"
echo "================================================================"
echo ""

# ν™κ²½ ν™•μΈ
if [[ -z "${VIRTUAL_ENV:-}" ]]; then
  echo "[ERROR] κ°€μƒν™κ²½μ΄ ν™μ„±ν™”λμ§€ μ•μ•μµλ‹λ‹¤." >&2
  echo "μ‹¤ν–‰: source /mnt/sdc1/ws/workspace/.venv_indextts/bin/activate" >&2
  exit 1
fi

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
CHECKPOINT_DIR="${CHECKPOINT_DIR:-/mnt/sda1/models/index-tts-ko/checkpoints}"
RESUME_PATH="${RESUME_PATH:-${CHECKPOINT_DIR}/latest.pth}"
DATASET_DIR="${DATASET_DIR:-/mnt/sda1/emilia-yodas/KO_preprocessed}"
RAW_MANIFEST="${RAW_MANIFEST:-/mnt/sda1/emilia-yodas/KO/ko_manifest_raw.jsonl}"

# Worker profile: default=auto, low=NUM_WORKERS=0, med=NUM_WORKERS=2
WORKER_PROFILE="${1:-auto}"
if [[ $# -gt 0 ]]; then
  shift
fi

SKIP_DATA_CHECK="${SKIP_DATA_CHECK:-1}"

# μ²΄ν¬ν¬μΈνΈ κ²€μ¦
if [[ ! -f "${RESUME_PATH}" ]]; then
  echo "[ERROR] μ²΄ν¬ν¬μΈνΈλ¥Ό μ°Ύμ„ μ μ—†μµλ‹λ‹¤: ${RESUME_PATH}" >&2
  exit 1
fi

CKPT_SIZE=$(stat -c%s "${RESUME_PATH}" 2>/dev/null || stat -f%z "${RESUME_PATH}" 2>/dev/null)
CKPT_SIZE_GB=$((CKPT_SIZE / 1024 / 1024 / 1024))

echo "π“ μ²΄ν¬ν¬μΈνΈ μ •λ³΄:"
echo "  - κ²½λ΅: ${RESUME_PATH}"
echo "  - ν¬κΈ°: ${CKPT_SIZE_GB}GB"
echo "  - μμ •: $(ls -lh "${RESUME_PATH}" | awk '{print $6, $7, $8}')"
echo ""

# GPU ν™•μΈ
GPU_MEM=$(nvidia-smi --query-gpu=memory.total --format=csv,noheader,nounits | head -1)
GPU_NAME=$(nvidia-smi --query-gpu=name --format=csv,noheader | head -1)

echo "β… GPU: ${GPU_NAME}"
echo "β… VRAM: ${GPU_MEM}MB"
echo ""

# λ°μ΄ν„° λ¬΄κ²°μ„± μ²΄ν¬ (μµμ…)
if [[ "${SKIP_DATA_CHECK}" != "1" ]]; then
  echo "π” λ°μ΄ν„° λ¬΄κ²°μ„± κ²€μ‚¬ μ¤‘..." >&2
  DATASET_DIR="${DATASET_DIR}" RAW_MANIFEST="${RAW_MANIFEST}" \
    "${SCRIPT_DIR}/ko_step2_fix_broken.sh" --scan-empty --scan-missing
else
  echo "β­οΈ  λ°μ΄ν„° κ²€μ‚¬ μƒλµ (SKIP_DATA_CHECK=1)" >&2
fi

# Worker profile μ„¤μ •
case "${WORKER_PROFILE}" in
  low)
    export NUM_WORKERS=0
    export OMP_NUM_THREADS="${OMP_NUM_THREADS:-2}"
    export MKL_NUM_THREADS="${MKL_NUM_THREADS:-2}"
    export TORCH_NUM_THREADS="${TORCH_NUM_THREADS:-2}"
    echo "β™οΈ  Worker profile: low (NUM_WORKERS=0, threads=2)" >&2
    ;;
  med)
    export NUM_WORKERS=2
    export OMP_NUM_THREADS="${OMP_NUM_THREADS:-4}"
    export MKL_NUM_THREADS="${MKL_NUM_THREADS:-4}"
    export TORCH_NUM_THREADS="${TORCH_NUM_THREADS:-4}"
    echo "β™οΈ  Worker profile: med (NUM_WORKERS=2, threads=4)" >&2
    ;;
  auto|*)
    echo "β™οΈ  Worker profile: auto (ν™κ²½λ³€μ μ μ§€)" >&2
    ;;
esac

echo ""
echo "π“ μµμ ν™” μ„¤μ • (Transformer λ…Όλ¬Έ κΈ°λ°):"
echo "  - Batch Size: 8 (RTX 4090 κ²€μ¦λ¨)"
echo "  - Gradient Accumulation: 8 (μ‹¤ν¨ batch 64)"
echo "  - Tokens/Batch: ~34,694 (Transformer λ…Όλ¬Έ μμ¤€)"
echo "  - Learning Rate: 1e-5"
echo "  - Warmup Steps: 30,000"
echo "  - Gradient Clip: 0.5"
echo "  - Epochs: 2"
echo ""
echo "π― κΈ°λ€ ν¨κ³Ό:"
echo "  - mel_loss variance λ€ν­ κ°μ† (~70%)"
echo "  - Training loss μ•μ •ν™”"
echo "  - Validation loss μ§€μ† κ°μ†"
echo "  - Transformer λ…Όλ¬Έ κΈ°μ¤€ λ‹¬μ„± β…"
echo "  - RTX 4090 24GB λ©”λ¨λ¦¬ μ•μ „ β…"
echo ""

# μ‚¬μ©μ ν™•μΈ
read -p "ν•™μµμ„ μ¬κ°ν•μ‹κ² μµλ‹κΉ? (y/N): " -n 1 -r
echo
if [[ ! $REPLY =~ ^[Yy]$ ]]; then
  echo "μ·¨μ†λμ—μµλ‹λ‹¤."
  exit 0
fi

echo ""
echo "π¬ ν•™μµμ„ μ¬κ°ν•©λ‹λ‹¤..."
echo "π“ TensorBoard: http://localhost:6006"
echo "π“ μ²΄ν¬ν¬μΈνΈ: ${CHECKPOINT_DIR}/"
echo ""

# μµμ ν™”λ μ„¤μ •μΌλ΅ μ¬κ° (Transformer λ…Όλ¬Έ κΈ°λ°, RTX 4090 μ•μ „)
SKIP_DATA_CHECK=1 \
LR=1e-5 \
WARMUP_STEPS=30000 \
BATCH_SIZE=8 \
GRAD_ACC=8 \
GRAD_CLIP=0.5 \
LOG_INTERVAL=100 \
VAL_INTERVAL=1000 \
EPOCHS=2 \
BASE_CHECKPOINT="/mnt/sda1/models/IndexTTS-2/gpt.pth" \
RESUME="${RESUME_PATH}" \
"${SCRIPT_DIR}/ko_step4_train_gpt.sh" "$@"

echo ""
echo "================================================================"
echo "β… ν•™μµ μ¬κ° μ™„λ£!"
echo "================================================================"
echo ""
echo "π“ μ €μ¥λ μ²΄ν¬ν¬μΈνΈ:"
echo "  - μµκ³  μ„±λ¥: ${CHECKPOINT_DIR}/best_model.pth"
echo "  - μµμ‹ : ${CHECKPOINT_DIR}/latest.pth"
echo ""
if [[ -f "${CHECKPOINT_DIR}/best_loss.txt" ]]; then
  echo "π† Best mel_loss: $(cat "${CHECKPOINT_DIR}/best_loss.txt")"
fi
echo ""
echo "λ‹¤μ λ‹¨κ³„:"
echo "  1. TensorBoardλ΅ ν•™μµ κ³΅μ„  ν™•μΈ"
echo "  2. mel_loss variance κ°μ† ν™•μΈ (μ΄μ „ λ€λΉ„ ~70% κ°μ„  μμƒ)"
echo "  3. Step 30k warmup μ™„λ£ ν›„ μ•μ •ν™” ν‰κ°€"
echo "  4. λ‹¤μ ν•™μµ μ‹ warmup 5000μΌλ΅ λ‹¨μ¶• κ³ λ ¤"
echo ""
