# IndexTTS2 3-Stage Training ì™„ë²½ ê°€ì´ë“œ

> **IndexTTS2**: Zero-shot Voice Cloning with Emotion Disentanglement
>
> arXiv:2506.21619v2 (2025)

## ëª©ì°¨
1. [IndexTTS2ë€?](#indexts2ë€)
2. [ì™œ 3-Stage Trainingì¸ê°€?](#ì™œ-3-stage-trainingì¸ê°€)
3. [Stage 1: Basic TTS Training](#stage-1-basic-tts-training)
4. [Stage 2: Emotion Disentanglement with GRL](#stage-2-emotion-disentanglement-with-grl)
5. [Stage 3: Fine-tuning](#stage-3-fine-tuning)
6. [ì‹¤ìŠµ ê°€ì´ë“œ](#ì‹¤ìŠµ-ê°€ì´ë“œ)
7. [ì´ë¡  ì‹¬í™”](#ì´ë¡ -ì‹¬í™”)
8. [FAQ & Troubleshooting](#faq--troubleshooting)

---

## IndexTTS2ë€?

### ê°œìš”
IndexTTS2ëŠ” **zero-shot voice cloning** ëª¨ë¸ë¡œ, ë‹¨ ëª‡ ì´ˆì˜ ìŒì„± ìƒ˜í”Œë§Œìœ¼ë¡œ ìƒˆë¡œìš´ í™”ìì˜ ëª©ì†Œë¦¬ë¥¼ ë³µì œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### í•µì‹¬ íŠ¹ì§•
1. **Zero-shot**: í•™ìŠµ ë•Œ ë³¸ ì  ì—†ëŠ” í™”ìì˜ ëª©ì†Œë¦¬ë„ ë³µì œ ê°€ëŠ¥
2. **Emotion Control**: í™”ìì™€ ë¬´ê´€í•˜ê²Œ ê°ì • í‘œí˜„ ì œì–´
3. **3-Stage Training**: ì²´ê³„ì ì¸ ë‹¨ê³„ë³„ í•™ìŠµ ì „ëµ

### ì•„í‚¤í…ì²˜
```
Input Text
    â†“
[Text Encoder]
    â†“
[GPT Backbone] â† [Speaker Condition] + [Emotion Condition]
    â†“
Semantic Codes
    â†“
[Vocoder]
    â†“
Audio
```

**Key Components**:
- **Speaker Perceiver**: í™”ìì˜ ìŒìƒ‰/ìŠ¤íƒ€ì¼ ì¶”ì¶œ
- **Emotion Perceiver**: ê°ì • í‘œí˜„ ì¶”ì¶œ
- **GPT Backbone**: Text â†’ Semantic codes ìƒì„±
- **Vocoder**: Semantic codes â†’ Audio

---

## ì™œ 3-Stage Trainingì¸ê°€?

### ë¬¸ì œ ìƒí™©
**ì´ˆê¸° í•™ìŠµ ì‹œ ë¬¸ì œì **:
```python
# âŒ í•œ ë²ˆì— ëª¨ë“  ê²ƒì„ í•™ìŠµí•˜ë©´?
speaker_feature + emotion_feature â†’ GPT â†’ output

ë¬¸ì œ 1: Speakerì™€ Emotionì´ ì—‰í‚´ (entanglement)
  - ê°ì • Aë¥¼ í™”ì Xì—ê²Œì„œë§Œ í•™ìŠµ
  - í™”ì Yì—ê²Œ ê°ì • Aë¥¼ ì ìš©í•˜ë©´ í™”ì Xì²˜ëŸ¼ ë“¤ë¦¼

ë¬¸ì œ 2: Feature extractionê³¼ generationì´ ë™ì‹œ í•™ìŠµ
  - ë‘ ê³¼ì œê°€ ì„œë¡œ ë°©í•´
  - ìµœì í™” ì–´ë ¤ì›€
```

### í•´ê²°ì±…: 3-Stage Training
```
Stage 1: ê¸°ë³¸ TTS ëŠ¥ë ¥ í•™ìŠµ
  â†’ ëª¨ë“  ì»´í¬ë„ŒíŠ¸ê°€ í˜‘ë ¥í•˜ì—¬ ìŒì„± ìƒì„± í•™ìŠµ

Stage 2: Speaker-Emotion Disentanglement
  â†’ Emotionì—ì„œ Speaker ì •ë³´ ì œê±°
  â†’ ì–´ë–¤ í™”ìì—ê²Œë“  ê°ì • ì ìš© ê°€ëŠ¥

Stage 3: Fine-tuning
  â†’ FeatureëŠ” ê³ ì •, ìƒì„± í’ˆì§ˆë§Œ ê°œì„ 
  â†’ Overfitting ë°©ì§€
```

---

## Stage 1: Basic TTS Training

### ëª©ì 
**"ê¸°ë³¸ì ì¸ Text-to-Speech ëŠ¥ë ¥ í•™ìŠµ"**

### í•™ìŠµ ì„¤ì •
```yaml
Dataset: ì „ì²´ ë°ì´í„° (2.7M samples)
Trainable: ëª¨ë“  ì»´í¬ë„ŒíŠ¸
  - Speaker Perceiver âœ…
  - Emotion Perceiver âœ…
  - GPT Backbone âœ…
Learning Rate: 2e-4
Epochs: ì—¬ëŸ¬ epoch (ìˆ˜ë ´í•  ë•Œê¹Œì§€)
```

### í•™ìŠµ ê³¼ì •
```python
# Pseudo-code
for batch in dataloader:
    # 1. Feature extraction
    speaker_feat = speaker_perceiver(speaker_mel)
    emotion_feat = emotion_perceiver(emotion_mel)

    # 2. Combine features
    condition = speaker_feat + emotion_feat

    # 3. Generate semantic codes
    codes = gpt(text, condition)

    # 4. Compute loss
    loss = cross_entropy(codes, target_codes)
    loss.backward()
    optimizer.step()
```

### í•™ìŠµ ê²°ê³¼
âœ… Text â†’ Semantic codes ë§¤í•‘ í•™ìŠµ
âœ… Speaker feature extraction í•™ìŠµ
âœ… Emotion feature extraction í•™ìŠµ
âŒ **í•˜ì§€ë§Œ**: Speakerì™€ Emotionì´ entangled (ì„ì—¬ìˆìŒ)

### ì‹¤í–‰ ë°©ë²•
```bash
cd /mnt/sdc1/ws/workspace/monorepo/external/index-tts
source /mnt/sdc1/ws/workspace/.venv_indextts/bin/activate

./tools/train_ko_optimized_4090.sh
```

### ëª¨ë‹ˆí„°ë§
```bash
tensorboard --logdir=/mnt/sda1/models/index-tts-ko/logs

# í™•ì¸í•  ì§€í‘œ:
# - train/mel_loss: ê°ì†Œí•´ì•¼ í•¨
# - train/mel_top1: ì¦ê°€í•´ì•¼ í•¨ (accuracy)
# - val/mel_loss: trainê³¼ ë¹„ìŠ·í•˜ê²Œ ê°ì†Œ (overfitting ì²´í¬)
```

---

## Stage 2: Emotion Disentanglement with GRL

### ëª©ì 
**"Emotion featureì—ì„œ Speaker ì •ë³´ ì œê±°"**

### ë¬¸ì œ ìƒí™©
Stage 1 í›„ ìƒíƒœ:
```python
emotion_vec = emotion_perceiver(audio)
# âŒ ë¬¸ì œ: emotion_vecì— speaker ì •ë³´ê°€ ì„ì—¬ìˆìŒ

# ì˜ˆì‹œ:
speaker_A + emotion_happy â†’ "í–‰ë³µí•œ Aì˜ ëª©ì†Œë¦¬"
speaker_B + emotion_happy â†’ "ì—¬ì „íˆ Aì²˜ëŸ¼ ë“¤ë¦¼" (âŒ)
```

### í•´ê²°ì±…: GRL (Gradient Reversal Layer)

#### GRL ì›ë¦¬
```python
class GradientReversalLayer:
    def forward(self, x):
        return x  # Forward: ê·¸ëŒ€ë¡œ í†µê³¼

    def backward(self, grad):
        return -lambda * grad  # Backward: gradient ë°˜ì „!
```

#### Adversarial Training
```
Emotion Encoderì˜ ëª©í‘œ:
  - ê°ì • ì •ë³´ëŠ” ì˜ ì¶”ì¶œí•˜ê³ 
  - Speaker ì •ë³´ëŠ” ì œê±°í•˜ê³  ì‹¶ìŒ

Speaker Classifierì˜ ëª©í‘œ:
  - Emotion vectorë¡œë¶€í„° speaker ë¶„ë¥˜

GRLì˜ ì—­í• :
  - Speaker ClassifierëŠ” ì •ìƒì ìœ¼ë¡œ í•™ìŠµ (speaker ë¶„ë¥˜ ì˜í•˜ë ¤ê³  í•¨)
  - Emotion EncoderëŠ” reversed gradientë¥¼ ë°›ìŒ
    â†’ Speaker Classifierë¥¼ "ì†ì´ë ¤ê³ " í•™ìŠµ
    â†’ ê²°ê³¼ì ìœ¼ë¡œ speaker ì •ë³´ ì œê±°!
```

### ìƒì„¸ êµ¬í˜„

#### 1. Forward Pass (Real-time Emo Vec Computation)
```python
# âœ… ì´ìƒì ì¸ ë°©ì‹ (ë…¼ë¬¸ê³¼ ë™ì¼)
condition = load_mel_spectrogram()  # [batch, cond_len, 1024]

# Real-timeìœ¼ë¡œ emo_vec ê³„ì‚°
emo_features = emo_conditioning_encoder(condition)
emo_vec_raw = emo_perceiver_encoder(emo_features)
emo_vec = emo_layer(emovec_layer(emo_vec_raw))

# Gradientê°€ ì—¬ê¸°ë¡œ íë¦„! â†“
```

#### 2. GRL + Speaker Classification
```python
# Apply GRL
emo_vec_reversed = GRL(emo_vec)

# Speaker classification
speaker_logits = speaker_classifier(emo_vec_reversed)
speaker_loss = cross_entropy(speaker_logits, speaker_labels)
```

#### 3. Backward Pass
```python
# Total loss
total_loss = tts_loss + alpha * speaker_loss

# Backward
total_loss.backward()

# Gradient flow:
# tts_loss â†’ GPT, emotion encoder (ì •ìƒ)
# speaker_loss â†’ speaker_classifier (ì •ìƒ)
#             â†’ GRL (reversed!) â†’ emotion encoder
#
# Emotion encoderëŠ”:
# - TTS lossë¡œë¶€í„°: ê°ì • ì •ë³´ ì¶”ì¶œí•˜ë¼
# - Speaker lossë¡œë¶€í„°: Speaker ì •ë³´ ì œê±°í•˜ë¼ (reversed gradient)
```

### í•™ìŠµ ì„¤ì •
```yaml
Dataset: ì „ì²´ ë°ì´í„° (ë˜ëŠ” ê°ì • ë°ì´í„° 135ì‹œê°„)
Trainable:
  - Speaker Perceiver: âŒ FROZEN
  - Emotion Perceiver: âœ… TRAINABLE
  - GPT Backbone: âœ… TRAINABLE
  - GRL + Speaker Classifier: âœ… TRAINABLE
Learning Rate: 2e-4
GRL Lambda: 1.0
Speaker Loss Weight: 0.1
Epochs: 2 (ë…¼ë¬¸ ê¶Œì¥)
```

### í•µì‹¬ ì½”ë“œ ë¶„ì„
```python
# trainers/train_gpt_v2.pyì˜ compute_losses()

# Real-time emo_vec computation
if enable_stage2_realtime_emo and model.enable_grl:
    # condition â†’ emo encoder â†’ emo_vec (ì‹¤ì‹œê°„ ê³„ì‚°)
    condition_transposed = condition.transpose(1, 2)
    emo_features = model.emo_conditioning_encoder(
        condition_transposed, condition_lengths
    )
    emo_vec_raw = model.emo_perceiver_encoder(emo_features)
    emo_vec = model.emo_layer(model.emovec_layer(emo_vec_raw.squeeze(1)))

    # GRL ì ìš©
    emo_vec_reversed = model.grl(emo_vec)
    speaker_logits = model.speaker_classifier(emo_vec_reversed)

    # Speaker loss ê³„ì‚°
    speaker_loss = F.cross_entropy(speaker_logits, speaker_labels)
```

### ì‹¤í–‰ ë°©ë²•

#### Step 1: Speaker Mapping ìƒì„±
```bash
python tools/build_speaker_mapping.py \
    --manifest /mnt/sda1/emilia-yodas/KO_preprocessed/train_manifest.jsonl \
    --output /mnt/sda1/models/index-tts-ko/speaker_mapping.json \
    --top-k 500 \
    --min-samples 50

# ì¶œë ¥:
# Total speakers: 132,389
# Selected top 500 speakers: 167,734 samples (6.03%)
```

#### Step 2: Stage 2 í•™ìŠµ
```bash
# Stage 1 checkpoint í™•ì¸
export STAGE1_CHECKPOINT=/mnt/sda1/models/index-tts-ko/checkpoints/best_model.pth

# Stage 2 ì‹¤í–‰
./tools/train_ko_stage2.sh
```

### ëª¨ë‹ˆí„°ë§
```bash
tensorboard --logdir=/mnt/sda1/models/index-tts-ko/stage2/logs

# ì¤‘ìš” ì§€í‘œ:
# - train/speaker_loss: ê°ì†Œí•´ì•¼ í•¨
# - train/speaker_acc: 30-60% ìœ ì§€ (ì¤‘ìš”!)
#   â†’ ë„ˆë¬´ ë†’ìœ¼ë©´ (>80%): GRLì´ íš¨ê³¼ ì—†ìŒ, emotionì— ì—¬ì „íˆ speaker ì •ë³´ ë§ìŒ
#   â†’ ë„ˆë¬´ ë‚®ìœ¼ë©´ (<20%): speaker classifierê°€ í•™ìŠµ ì•ˆë¨
# - train/mel_loss: Stage 1ê³¼ ë¹„ìŠ·í•˜ê²Œ ìœ ì§€
```

### ì„±ê³µ ê¸°ì¤€
âœ… Speaker accuracy: 30-60% (randomë³´ë‹¤ëŠ” ë†’ì§€ë§Œ ë„ˆë¬´ ë†’ì§€ ì•ŠìŒ)
âœ… Mel loss: Stage 1ê³¼ ë¹„ìŠ· (TTS í’ˆì§ˆ ìœ ì§€)
âœ… Emotion transfer: ë‹¤ë¥¸ í™”ìì—ê²Œ ê°ì • ì ìš© ì‹œ ì›ë˜ ê°ì • ìœ ì§€

---

## Stage 3: Fine-tuning

### ëª©ì 
**"FeatureëŠ” ë³´ì¡´í•˜ë©´ì„œ ìƒì„± í’ˆì§ˆë§Œ ê°œì„ "**

### ì™œ í•„ìš”í•œê°€?

#### ë¬¸ì œ ìƒí™©
Stage 2 í›„:
```python
# âœ… ì¢‹ì€ ì :
# - Speaker feature ì˜ ì¶”ì¶œë¨ (Stage 1)
# - Emotion feature ì˜ ë¶„ë¦¬ë¨ (Stage 2)

# âŒ ë¬¸ì œ:
# - ê³„ì† í•™ìŠµí•˜ë©´ feature drift ë°œìƒ ê°€ëŠ¥
# - Speaker/Emotion perceiverê°€ ë³€í•˜ë©´ Stage 2ì˜ disentanglement ë§ê°€ì§
```

#### í•´ê²°ì±…: Freeze Conditioners
```python
# Feature extractors ê³ ì •
speaker_perceiver.requires_grad = False  # ğŸ”’
emotion_perceiver.requires_grad = False  # ğŸ”’

# GPTë§Œ í•™ìŠµ
gpt.requires_grad = True  # âœ…

# ê²°ê³¼:
# - Stage 1, 2ì˜ feature ë³´ì¡´
# - GPTì˜ ìƒì„± í’ˆì§ˆë§Œ ê°œì„ 
# - Overfitting ë°©ì§€
```

### í•™ìŠµ ì„¤ì •
```yaml
Dataset: ì „ì²´ ë°ì´í„°
Frozen (ğŸ”’):
  - Speaker conditioning encoder
  - Speaker perceiver encoder
  - Emotion conditioning encoder
  - Emotion perceiver encoder
  - Emovec layer
  - Emo layer
Trainable (âœ…):
  - GPT Backbone
  - Text/Mel embeddings
  - Text/Mel heads
Learning Rate: 1e-4 (Stage 1/2ì˜ ì ˆë°˜!)
Epochs: 1
GRL: Disabled
```

### í•µì‹¬ ì½”ë“œ
```python
# trainers/train_gpt_v2.pyì˜ main()

if args.freeze_conditioners:
    # Freeze all feature extractors
    for module in [
        model.conditioning_encoder,
        model.perceiver_encoder,
        model.emo_conditioning_encoder,
        model.emo_perceiver_encoder,
        model.emovec_layer,
        model.emo_layer
    ]:
        for param in module.parameters():
            param.requires_grad = False

    # ê²°ê³¼ í™•ì¸
    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
    total = sum(p.numel() for p in model.parameters())
    print(f"Trainable: {trainable:,} / {total:,} ({100*trainable/total:.1f}%)")
    # ì¶œë ¥ ì˜ˆì‹œ: Trainable: 45,234,567 / 98,765,432 (45.8%)
```

### ì‹¤í–‰ ë°©ë²•
```bash
# Stage 2 checkpoint í™•ì¸
export STAGE2_CHECKPOINT=/mnt/sda1/models/index-tts-ko/stage2/checkpoints/best_model.pth

# Stage 3 ì‹¤í–‰
./tools/train_ko_stage3.sh
```

### ëª¨ë‹ˆí„°ë§
```bash
tensorboard --logdir=/mnt/sda1/models/index-tts-ko/stage3/logs

# í™•ì¸ ì‚¬í•­:
# - train/mel_loss: ë¯¸ì„¸í•˜ê²Œ ê°ì†Œ (í° ê°œì„  ê¸°ëŒ€í•˜ì§€ ë§ê²ƒ)
# - Trainable params: ~40-50%ë¡œ ê°ì†Œ í™•ì¸
# - í•™ìŠµ ì†ë„: Stage 1/2ë³´ë‹¤ ~2ë°° ë¹ ë¦„
```

### ì„±ê³µ ê¸°ì¤€
âœ… Mel loss: Stage 2ì™€ ë¹„ìŠ·í•˜ê±°ë‚˜ ì•½ê°„ ê°œì„ 
âœ… Speaker similarity: ìœ ì§€
âœ… Emotion transfer: ìœ ì§€ (Stage 2ì˜ ê²°ê³¼ ë³´ì¡´)
âœ… í•™ìŠµ ì•ˆì •ì„±: Lossê°€ íŠ€ì§€ ì•ŠìŒ (frozen features ë•ë¶„)

---

## ì‹¤ìŠµ ê°€ì´ë“œ

### ì „ì²´ ì›Œí¬í”Œë¡œìš°

#### 0. í™˜ê²½ ì¤€ë¹„
```bash
# 1. ê°€ìƒí™˜ê²½ í™œì„±í™”
source /mnt/sdc1/ws/workspace/.venv_indextts/bin/activate

# 2. í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ ì´ë™
cd /mnt/sdc1/ws/workspace/monorepo/external/index-tts

# 3. GPU í™•ì¸
nvidia-smi

# 4. TensorBoard ì‹¤í–‰ (ë³„ë„ í„°ë¯¸ë„)
tensorboard --logdir=/mnt/sda1/models/index-tts-ko
# ë¸Œë¼ìš°ì €: http://localhost:6006
```

#### 1. Stage 1 í•™ìŠµ
```bash
# ì‹¤í–‰
./tools/train_ko_optimized_4090.sh

# ì†Œìš” ì‹œê°„: ìˆ˜ì¼ (ë°ì´í„°ì…‹ í¬ê¸°ì— ë”°ë¼)

# ëª¨ë‹ˆí„°ë§ (TensorBoard)
# - train/mel_loss: ì§€ì†ì  ê°ì†Œ
# - train/mel_top1: 0.6-0.8 ì •ë„ ë„ë‹¬
# - val/mel_loss: trainê³¼ ë¹„ìŠ·í•˜ê²Œ ê°ì†Œ

# ì™„ë£Œ ì¡°ê±´:
# - mel_lossê°€ ìˆ˜ë ´ (ë” ì´ìƒ ê°ì†Œ ì•ˆí•¨)
# - ìƒì„±ëœ ì˜¤ë””ì˜¤ í’ˆì§ˆ acceptable

# Checkpoint ìœ„ì¹˜:
# /mnt/sda1/models/index-tts-ko/checkpoints/best_model.pth
```

#### 2. Speaker Mapping ìƒì„± (í•œ ë²ˆë§Œ)
```bash
python tools/build_speaker_mapping.py \
    --manifest /mnt/sda1/emilia-yodas/KO_preprocessed/train_manifest.jsonl \
    --output /mnt/sda1/models/index-tts-ko/speaker_mapping.json \
    --top-k 500 \
    --min-samples 50

# ì¶œë ¥ ì˜ˆì‹œ:
Building speaker mapping from /mnt/sda1/emilia-yodas/KO_preprocessed/train_manifest.jsonl
Total samples: 2,783,826
Total unique speakers: 132,389
Speakers with >= 50 samples: 14,244
Selected top 500 speakers: 167,734 samples (6.03% of total)

Speaker mapping saved to /mnt/sda1/models/index-tts-ko/speaker_mapping.json

# ê²€ì¦:
cat /mnt/sda1/models/index-tts-ko/speaker_mapping.json | jq 'length'
# ì¶œë ¥: 500
```

#### 3. Stage 2 í•™ìŠµ
```bash
# Stage 1 checkpoint ê²½ë¡œ ì„¤ì •
export STAGE1_CHECKPOINT=/mnt/sda1/models/index-tts-ko/checkpoints/best_model.pth

# ì‹¤í–‰
./tools/train_ko_stage2.sh

# ì†Œìš” ì‹œê°„: 1-2 epochs

# ëª¨ë‹ˆí„°ë§ (TensorBoard)
# ğŸ¯ í•µì‹¬ ì§€í‘œ: train/speaker_acc
# - ì´ìƒì : 30-60%
# - ë„ˆë¬´ ë†’ìŒ (>80%): GRL lambda ì¦ê°€ (1.0 â†’ 2.0)
# - ë„ˆë¬´ ë‚®ìŒ (<20%): Speaker loss weight ì¦ê°€ (0.1 â†’ 0.2)

# ì™„ë£Œ ì¡°ê±´:
# - Speaker accuracy 30-60% ì•ˆì •í™”
# - Mel loss Stage 1ê³¼ ë¹„ìŠ·
# - 2 epochs ì™„ë£Œ

# Checkpoint ìœ„ì¹˜:
# /mnt/sda1/models/index-tts-ko/stage2/checkpoints/best_model.pth
```

#### 4. Stage 3 í•™ìŠµ
```bash
# Stage 2 checkpoint ê²½ë¡œ ì„¤ì •
export STAGE2_CHECKPOINT=/mnt/sda1/models/index-tts-ko/stage2/checkpoints/best_model.pth

# ì‹¤í–‰
./tools/train_ko_stage3.sh

# ì†Œìš” ì‹œê°„: 1 epoch

# ì‹œì‘ ì‹œ ì¶œë ¥ í™•ì¸:
[Stage 3] Freezing feature conditioners...
  âœ… Speaker conditioning encoder frozen
  âœ… Speaker perceiver encoder frozen
  âœ… Emotion conditioning encoder frozen
  âœ… Emotion perceiver encoder frozen
  âœ… Emovec layer frozen
  âœ… Emo layer frozen
[Stage 3] Trainable parameters: 45,234,567 / 98,765,432 (45.8%)

# ëª¨ë‹ˆí„°ë§:
# - í•™ìŠµ ì†ë„: Stage 1/2ë³´ë‹¤ ë¹ ë¦„ (50% params)
# - Mel loss: ë¯¸ì„¸ ê°œì„  ë˜ëŠ” ìœ ì§€
# - ì•ˆì •ì„±: Lossê°€ íŠ€ì§€ ì•ŠìŒ

# ìµœì¢… Checkpoint:
# /mnt/sda1/models/index-tts-ko/stage3/checkpoints/best_model.pth
```

### í•™ìŠµ ì¤‘ë‹¨ ë° ì¬ê°œ
```bash
# ê° stageëŠ” ìë™ìœ¼ë¡œ checkpoint ì €ì¥
# ì¬ê°œ ì‹œ --base-checkpointì— ë§ˆì§€ë§‰ checkpoint ì§€ì •

# ì˜ˆ: Stage 2 ì¬ê°œ
python trainers/train_gpt_v2.py \
    ... (ë‹¤ë¥¸ arguments) ... \
    --base-checkpoint /mnt/sda1/models/index-tts-ko/stage2/checkpoints/model_step_5000.pth
```

---

## ì´ë¡  ì‹¬í™”

### 1. GRL (Gradient Reversal Layer) ìˆ˜í•™ì  ì´í•´

#### Forward Pass
```python
y = GRL(x) = x  # Identity function
```

#### Backward Pass
```python
âˆ‚L/âˆ‚x = -Î» * âˆ‚L/âˆ‚y

ì—¬ê¸°ì„œ:
- L: Total loss
- Î»: Reversal strength (typically 1.0)
- âˆ‚L/âˆ‚y: Gradient from next layer
```

#### ì™œ ì‘ë™í•˜ëŠ”ê°€?

**ì¼ë°˜ì ì¸ í•™ìŠµ**:
```python
# Encoderì˜ ëª©í‘œ: Loss ìµœì†Œí™”
loss = f(encoder(x))
âˆ‚loss/âˆ‚encoder_params = âˆ‚loss/âˆ‚f * âˆ‚f/âˆ‚encoder  # ì •ìƒ gradient
encoder_params -= lr * âˆ‚loss/âˆ‚encoder_params  # Loss ê°ì†Œ ë°©í–¥ìœ¼ë¡œ ì—…ë°ì´íŠ¸
```

**GRL ì ìš©**:
```python
# Emotion Encoderì˜ ëª©í‘œ: Speaker classifierë¥¼ ì†ì´ê¸°
speaker_pred = classifier(GRL(emotion_encoder(x)))
loss = cross_entropy(speaker_pred, speaker_label)

# Backward:
âˆ‚loss/âˆ‚encoder_params = âˆ‚loss/âˆ‚classifier * âˆ‚classifier/âˆ‚GRL * (-Î») * âˆ‚GRL/âˆ‚encoder
                      = âˆ‚loss/âˆ‚classifier * âˆ‚classifier/âˆ‚GRL * (-Î») * âˆ‚encoder/âˆ‚encoder
                                                                   â†‘
                                                            Gradient reversed!

# ê²°ê³¼:
# - Classifier: speaker ì˜ ë¶„ë¥˜í•˜ë ¤ê³  í•™ìŠµ (ì •ìƒ gradient)
# - Encoder: speaker ëª» ë¶„ë¥˜í•˜ê²Œ ë§Œë“¤ë ¤ê³  í•™ìŠµ (reversed gradient)
```

### 2. Adversarial Trainingì˜ ê· í˜•

#### Min-Max Game
```
min_Î¸_emo max_Î¸_classifier L_speaker

ì—¬ê¸°ì„œ:
- Î¸_emo: Emotion encoder parameters
- Î¸_classifier: Speaker classifier parameters
- L_speaker: Speaker classification loss

Emotion encoderëŠ” Lì„ ìµœì†Œí™”í•˜ë ¤ í•˜ê³  (speaker ì œê±°)
ClassifierëŠ” Lì„ ìµœëŒ€í™”í•˜ë ¤ í•¨ (speaker ë¶„ë¥˜)
```

#### Nash Equilibrium
ì´ìƒì ì¸ ê· í˜•ì :
```python
Speaker Accuracy = 1/N  # Random guess level

ì—¬ê¸°ì„œ N = number of speakers

ì˜ˆ: 500 speakers â†’ ideal accuracy = 0.2% (random)
ì‹¤ì œ: 30-60% ì •ë„ë©´ ì¶©ë¶„íˆ ì¢‹ìŒ (perfect randomì€ ì–´ë ¤ì›€)
```

### 3. ì™œ Real-time Emo Vecì´ ì¤‘ìš”í•œê°€?

#### Pre-computed ë°©ì‹
```python
# Preprocessing
emo_vec = emotion_perceiver(mel).detach()  # Gradient ëŠê¹€!
save(emo_vec, "emo_vec.npy")

# Training (Stage 2)
emo_vec = load("emo_vec.npy")  # No gradient!
emo_vec_reversed = GRL(emo_vec)
loss.backward()
# âŒ Gradientê°€ emotion_perceiverë¡œ ëª» íë¦„!
```

#### Real-time ë°©ì‹
```python
# Training (Stage 2)
mel = load_mel_spectrogram()
emo_vec = emotion_perceiver(mel)  # âœ… Gradient ì‚´ì•„ìˆìŒ
emo_vec_reversed = GRL(emo_vec)
loss.backward()
# âœ… Gradientê°€ emotion_perceiverë¡œ íë¦„!
```

**ê²°ê³¼ ë¹„êµ**:
| ë°©ì‹ | Gradient Flow | íš¨ê³¼ | ì†ë„ |
|------|--------------|------|------|
| Pre-computed | âŒ | GRLì´ ì œëŒ€ë¡œ ì‘ë™ ì•ˆí•¨ | ë¹ ë¦„ |
| Real-time | âœ… | GRLì´ ì œëŒ€ë¡œ ì‘ë™ | ~10-15% ëŠë¦¼ |

### 4. Feature Freezingì˜ ìˆ˜í•™

#### Without Freezing
```python
# All parameters trainable
Î¸ = [Î¸_speaker, Î¸_emotion, Î¸_gpt]
loss = L_tts(y, y_target)
âˆ‚loss/âˆ‚Î¸ = [âˆ‚loss/âˆ‚Î¸_speaker, âˆ‚loss/âˆ‚Î¸_emotion, âˆ‚loss/âˆ‚Î¸_gpt]

# ë¬¸ì œ:
# - Stage 2ì—ì„œ í•™ìŠµí•œ Î¸_emotionì´ ë³€í•¨
# - Speaker-emotion disentanglement ë§ê°€ì§
```

#### With Freezing (Stage 3)
```python
# Only GPT trainable
Î¸_frozen = [Î¸_speaker, Î¸_emotion]  # Fixed
Î¸_trainable = [Î¸_gpt]  # Updated

loss = L_tts(y, y_target)
âˆ‚loss/âˆ‚Î¸_trainable = [âˆ‚loss/âˆ‚Î¸_gpt]  # Only this updates

# ì¥ì :
# 1. Î¸_speaker, Î¸_emotion ë³´ì¡´ (Stage 1, 2 ê²°ê³¼ ìœ ì§€)
# 2. í•™ìŠµ ì•ˆì •ì„± í–¥ìƒ (fewer parameters to optimize)
# 3. Overfitting ë°©ì§€
# 4. í•™ìŠµ ì†ë„ í–¥ìƒ (~50% parameters)
```

---

## FAQ & Troubleshooting

### General

#### Q: 3 stage ëª¨ë‘ í•„ìˆ˜ì¸ê°€ìš”?
**A**: ë…¼ë¬¸ì— ë”°ë¥´ë©´ ìµœìƒì˜ ê²°ê³¼ë¥¼ ìœ„í•´ 3 stage ëª¨ë‘ ê¶Œì¥í•©ë‹ˆë‹¤.
- Stage 1 only: ê¸°ë³¸ TTS ê°€ëŠ¥, ê°ì • ì œì–´ ì œí•œì 
- Stage 1+2: ê°ì • ì œì–´ ê°€ëŠ¥, overfitting ê°€ëŠ¥ì„±
- Stage 1+2+3: ìµœì  (ê°ì • ì œì–´ + ì•ˆì •ì„±)

#### Q: ê° stage í•™ìŠµ ì‹œê°„ì€?
**A**: ë°ì´í„°ì…‹ í¬ê¸°ì— ë”°ë¼ ë‹¤ë¦„ (2.7M samples ê¸°ì¤€)
- Stage 1: ~3-5ì¼ (ì—¬ëŸ¬ epoch)
- Stage 2: ~1-2ì¼ (2 epochs)
- Stage 3: ~12-24ì‹œê°„ (1 epoch)

#### Q: Stage 2ì—ì„œ speaker_accê°€ 80% ì´ìƒì…ë‹ˆë‹¤
**A**: GRLì´ íš¨ê³¼ê°€ ì—†ìŒ. ë‹¤ìŒ ì‹œë„:
1. GRL lambda ì¦ê°€: `export GRL_LAMBDA=2.0` ë˜ëŠ” `3.0`
2. Speaker loss weight ì¦ê°€: `export SPEAKER_LOSS_WEIGHT=0.2`
3. Learning rate í™•ì¸: ë„ˆë¬´ ë†’ìœ¼ë©´ GRL í•™ìŠµ ë¶ˆì•ˆì •

#### Q: Stage 2ì—ì„œ speaker_accê°€ 20% ì´í•˜ì…ë‹ˆë‹¤
**A**: Speaker classifierê°€ í•™ìŠµ ì•ˆë¨. ë‹¤ìŒ ì‹œë„:
1. Speaker mapping í™•ì¸: 500ê°œ speakerê°€ ì¶©ë¶„í•œê°€?
2. Speaker loss weight ì¦ê°€: `export SPEAKER_LOSS_WEIGHT=0.2`
3. Speaker mapping ì¬ìƒì„±: min_samples ë‚®ì¶”ê¸°

#### Q: Stage 3ì—ì„œ mel_lossê°€ ì¦ê°€í•©ë‹ˆë‹¤
**A**: Overfitting ë˜ëŠ” learning rate ê³¼ë‹¤. ë‹¤ìŒ ì‹œë„:
1. Learning rate ë‚®ì¶”ê¸°: `export LR=5e-5`
2. Epochs ì¤„ì´ê¸°: `export EPOCHS=0.5` (half epoch)
3. Early stopping: mel_loss ì¦ê°€ ì‹œ ì¤‘ë‹¨

### Stage 1 Issues

#### Q: Out of memory error
**A**: Batch size ë˜ëŠ” gradient accumulation ì¡°ì •
```bash
export BATCH_SIZE=4  # Default: 8
export GRAD_ACC=16   # Default: 8
# ì‹¤íš¨ batch size = 4 * 16 = 64 (ë™ì¼)
```

#### Q: mel_lossê°€ ìˆ˜ë ´í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤
**A**:
1. Learning rate í™•ì¸: ë„ˆë¬´ ë‚®ìœ¼ë©´ í•™ìŠµ ëŠë¦¼
2. Warmup steps ì¦ê°€: `export WARMUP_STEPS=10000`
3. Gradient clipping: `export GRAD_CLIP=1.0` (ë” í¬ê²Œ)

### Stage 2 Issues

#### Q: "Speaker mapping not found"
**A**: Speaker mapping ë¨¼ì € ìƒì„±:
```bash
python tools/build_speaker_mapping.py \
    --manifest /mnt/sda1/emilia-yodas/KO_preprocessed/train_manifest.jsonl \
    --output /mnt/sda1/models/index-tts-ko/speaker_mapping.json \
    --top-k 500
```

#### Q: Real-time emo vecì´ ë„ˆë¬´ ëŠë¦½ë‹ˆë‹¤
**A**: Fallback mode ì‚¬ìš© (pre-computed):
```bash
# train_ko_stage2.shì—ì„œ --enable-stage2-realtime-emo ì œê±°
# ë‹¨, GRL íš¨ê³¼ëŠ” ê°ì†Œ
```

#### Q: Speaker classification lossê°€ ë°œì‚°í•©ë‹ˆë‹¤
**A**:
1. Speaker loss weight ë‚®ì¶”ê¸°: `export SPEAKER_LOSS_WEIGHT=0.05`
2. GRL lambda ë‚®ì¶”ê¸°: `export GRL_LAMBDA=0.5`
3. Mixed precision ë¹„í™œì„±í™”: `--no-amp`

### Stage 3 Issues

#### Q: "Stage 2 checkpoint not found"
**A**: Stage 2ë¥¼ ë¨¼ì € ì™„ë£Œí•´ì•¼ í•©ë‹ˆë‹¤:
```bash
ls -lh /mnt/sda1/models/index-tts-ko/stage2/checkpoints/best_model.pth
# ì—†ìœ¼ë©´ Stage 2 ë¨¼ì € ì‹¤í–‰
```

#### Q: Frozen parametersê°€ ì œëŒ€ë¡œ ì ìš©ë˜ì—ˆëŠ”ì§€ í™•ì¸
**A**: í•™ìŠµ ì‹œì‘ ì‹œ ë¡œê·¸ í™•ì¸:
```
[Stage 3] Freezing feature conditioners...
  âœ… Speaker conditioning encoder frozen
  ...
[Stage 3] Trainable parameters: 45,234,567 / 98,765,432 (45.8%)
```

---

## ì°¸ê³  ìë£Œ

### ë…¼ë¬¸
1. **IndexTTS2**: [arXiv:2506.21619v2](https://arxiv.org/abs/2506.21619)
2. **GRL (Domain-Adversarial Training)**: Ganin et al., 2016, JMLR

### ì½”ë“œ
- **Implementation**: `/mnt/sdc1/ws/workspace/monorepo/external/index-tts/`
- **Training Scripts**: `tools/train_ko_stage*.sh`
- **Main Trainer**: `trainers/train_gpt_v2.py`

### ë¬¸ì„œ
- **STAGE2_IMPLEMENTATION.md**: ìƒì„¸ êµ¬í˜„ ë‚´ìš©
- **README.md**: í”„ë¡œì íŠ¸ ê°œìš”

---

## ìš”ì•½

### 3-Stage Training Flow
```
ğŸ“Š Prepare Data
    â†“
ğŸ¯ Stage 1: Basic TTS (ìˆ˜ì¼)
    â”œâ”€ All components trainable
    â”œâ”€ Learn text â†’ audio mapping
    â””â”€ Checkpoint: best_model.pth
    â†“
ğŸ”§ Build Speaker Mapping (1íšŒë§Œ)
    â”œâ”€ Select top 500 speakers
    â””â”€ Output: speaker_mapping.json
    â†“
ğŸ­ Stage 2: Emotion Disentanglement (1-2ì¼)
    â”œâ”€ Speaker perceiver frozen
    â”œâ”€ GRL + Real-time emo vec
    â”œâ”€ Speaker-emotion separation
    â””â”€ Checkpoint: stage2/best_model.pth
    â†“
ğŸ”’ Stage 3: Fine-tuning (12-24ì‹œê°„)
    â”œâ”€ All conditioners frozen
    â”œâ”€ GPT only trainable
    â”œâ”€ Quality refinement
    â””â”€ Checkpoint: stage3/best_model.pth
    â†“
âœ… Final Model
```

### Key Commands
```bash
# Environment
source /mnt/sdc1/ws/workspace/.venv_indextts/bin/activate
cd /mnt/sdc1/ws/workspace/monorepo/external/index-tts

# Stage 1
./tools/train_ko_optimized_4090.sh

# Speaker Mapping (once)
python tools/build_speaker_mapping.py --manifest ... --output ...

# Stage 2
./tools/train_ko_stage2.sh

# Stage 3
./tools/train_ko_stage3.sh

# Monitor
tensorboard --logdir=/mnt/sda1/models/index-tts-ko
```

### Success Criteria
| Stage | Key Metric | Target |
|-------|-----------|--------|
| Stage 1 | mel_loss | < 2.0, converged |
| Stage 1 | mel_top1 | > 0.6 |
| Stage 2 | speaker_acc | 30-60% |
| Stage 2 | mel_loss | Similar to Stage 1 |
| Stage 3 | mel_loss | Similar or better than Stage 2 |
| Stage 3 | trainable% | ~40-50% |

---

**Happy Training! ğŸš€**

*Last updated: 2025-11-19*
