# IndexTTS-2 Korean Training Optimizations

## ğŸ“… ì ìš© ë‚ ì§œ: 2025-11-21

ì´ ë¬¸ì„œëŠ” IndexTTS-2 í•œêµ­ì–´ fine-tuning í•™ìŠµì— ì ìš©í•œ ëª¨ë“  ìµœì í™”ë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤.

---

## ğŸ¯ ìµœì í™” ëª©í‘œ

- **í•™ìŠµ ì†ë„ í–¥ìƒ**: 240,000 step ì™„ë£Œ ì‹œê°„ ë‹¨ì¶• (86ì‹œê°„ â†’ 36ì‹œê°„ ì˜ˆìƒ)
- **ìë™ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°ì •**: ìˆ˜ë™ LR íŠœë‹ ì œê±°
- **ì•ˆì •ì ì¸ í•™ìŠµ**: BFloat16 + Prodigyë¡œ ì•ˆì •ì„± í–¥ìƒ
- **GPU í™œìš© ìµœëŒ€í™”**: RTX 4090 24GB ì™„ì „ í™œìš©

---

## âœ… ì ìš©ëœ ìµœì í™” ëª©ë¡

### 1. Prodigy Optimizer (í•µì‹¬)

**ë…¼ë¬¸**: "Prodigy: An Expeditiously Adaptive Parameter-Free Learner" (ICLR 2025)

**íš¨ê³¼**:
- âœ… ìë™ í•™ìŠµë¥  ì¡°ì • (LR scheduling ë¶ˆí•„ìš”)
- âœ… 10-15% ë¹ ë¥¸ ìˆ˜ë ´
- âœ… O(âˆšlog(D/dâ‚€)) ìˆ˜ë ´ ë³´ì¥ (ì´ë¡ ì ìœ¼ë¡œ ì¦ëª…ë¨)

**ì„¤ì •**:
```python
from prodigyopt import Prodigy

optimizer = Prodigy(
    model.parameters(),
    lr=1.0,                    # Prodigy's default (ìë™ ì¡°ì •ë¨)
    weight_decay=0.01,         # L2 regularization
    d_coef=1.0,                # Adaptivity coefficient
    use_bias_correction=False, # Stability
    safeguard_warmup=False,    # Stability
)
```

**ì£¼ìš” íŠ¹ì§•**:
- AdamWì™€ í˜¸í™˜ë˜ì§€ ì•ŠëŠ” optimizer state (checkpoint ì „í™˜ ì‹œ ì£¼ì˜)
- Phase 1/2 ë¶„ë¦¬ ë¶ˆí•„ìš” (ìë™ìœ¼ë¡œ LR ì¡°ì •)
- GPT, Transformer ë“± ëª¨ë“  differentiable lossì— ê²€ì¦ë¨

**ì„¤ì¹˜**:
```bash
uv pip install prodigyopt
```

---

### 2. torch.compile (PyTorch 2.0+)

**íš¨ê³¼**: 15-30% ì†ë„ í–¥ìƒ

**ì„¤ì •**:
```python
import torch

# Compile model for JIT optimization
model = torch.compile(model, mode="reduce-overhead")
```

**íŠ¹ì§•**:
- ì²« ë²ˆì§¸ ì‹¤í–‰ ì‹œ JIT compilation (10-20ë¶„ ì†Œìš”)
- ì´í›„ ëª¨ë“  stepì€ ìµœì í™”ëœ ì½”ë“œë¡œ ì‹¤í–‰
- Graph-level optimization

---

### 3. Flash Attention 2 (SDPA Backend)

**íš¨ê³¼**: 2-4Ã— ë¹ ë¥¸ attention, 50% ë©”ëª¨ë¦¬ ì ˆì•½

**ì„¤ì •**:
```python
import torch

if torch.cuda.is_available():
    torch.backends.cuda.enable_flash_sdp(True)
    torch.backends.cuda.enable_mem_efficient_sdp(True)
```

**íŠ¹ì§•**:
- PyTorch 2.0+ SDPA (Scaled Dot Product Attention) ìë™ ì‚¬ìš©
- Flash Attention 2 ì•Œê³ ë¦¬ì¦˜ í™œìš©
- ì¶”ê°€ ì„¤ì¹˜ ë¶ˆí•„ìš” (PyTorch ë‚´ì¥)

---

### 4. BFloat16 AMP (Automatic Mixed Precision)

**íš¨ê³¼**: FP16ê³¼ ë™ì¼í•œ ì†ë„, ë” ì•ˆì •ì ì¸ í•™ìŠµ

**ì„¤ì •**:
```python
import torch

# BFloat16 AMP (ë” ë„“ì€ dynamic range)
with torch.cuda.amp.autocast(enabled=True, dtype=torch.bfloat16):
    outputs = model(inputs)
```

**FP16 vs BFloat16**:
- FP16: 1 sign, 5 exponent, 10 mantissa (overflow/underflow ìœ„í—˜)
- **BFloat16**: 1 sign, 8 exponent, 7 mantissa (FP32ì™€ ë™ì¼í•œ range)
- Prodigyì™€ í•¨ê»˜ ì‚¬ìš© ì‹œ ë”ìš± ì•ˆì •ì 

---

### 5. cuDNN Benchmark + Matmul Precision

**íš¨ê³¼**: 5-10% (cuDNN) + 20-30% (matmul) ì†ë„ í–¥ìƒ

**ì„¤ì •**:
```python
import torch

# cuDNN auto-tuning
torch.backends.cudnn.benchmark = True

# High precision matmul (TF32 ì‚¬ìš©)
torch.set_float32_matmul_precision("high")
```

**íŠ¹ì§•**:
- cuDNN benchmark: ìµœì ì˜ convolution ì•Œê³ ë¦¬ì¦˜ ìë™ ì„ íƒ
- Matmul precision: TensorFloat-32 (TF32) í™œìš© (RTX 30xx/40xx)

---

### 6. DataLoader Optimizations

**íš¨ê³¼**: 20-30% ë¹ ë¥¸ ë°ì´í„° ë¡œë”©

**ì„¤ì •**:
```python
from torch.utils.data import DataLoader

dataloader_kwargs = {
    "persistent_workers": True,      # Worker ì¬ì‚¬ìš© (fork overhead ì œê±°)
    "prefetch_factor": 2,            # 2 batch ë¯¸ë¦¬ ì¤€ë¹„
    "multiprocessing_context": "fork", # ë¹ ë¥¸ fork (Linux)
}

train_loader = DataLoader(
    train_dataset,
    batch_size=8,
    shuffle=True,
    num_workers=32,
    pin_memory=True,
    **dataloader_kwargs,
)
```

**íŠ¹ì§•**:
- persistent_workers: Process ì¬ì‚¬ìš© (Python interpreter ì¬ì‹œì‘ ë¹„ìš© ì œê±°)
- prefetch_factor: GPU ëŒ€ê¸° ì‹œê°„ ìµœì†Œí™”
- num_workers=32: ìµœëŒ€ ë³‘ë ¬ ì²˜ë¦¬

---

## ğŸ“Š ì˜ˆìƒ ì„±ëŠ¥ í–¥ìƒ

| ìµœì í™” í•­ëª© | ê°œë³„ íš¨ê³¼ | ëˆ„ì  íš¨ê³¼ |
|------------|----------|----------|
| Prodigy Optimizer | 10-15% | 1.12Ã— |
| torch.compile | 15-30% | 1.40Ã— |
| Flash Attention 2 | 2-4Ã— attention | 1.96Ã— |
| cuDNN + Matmul | 25-40% | 2.35Ã— |
| DataLoader | 20-30% | **2.4Ã— (ìµœì¢…)** |

**ì˜ˆìƒ í•™ìŠµ ì‹œê°„**:
- AdamW (ê¸°ì¡´): 86ì‹œê°„ (240k steps)
- Prodigy (ìµœì í™”): **36ì‹œê°„ (240k steps)** âœ¨

---

## ğŸš€ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸

### ìƒˆë¡œ ì‹œì‘ (Step 0)

```bash
./tools/train_ko_prodigy.sh
```

**ì„¤ì •**:
- Optimizer: Prodigy
- Batch size: 8
- Grad accumulation: 1
- Max steps: 240,000
- Workers: 32

### ì¬ê°œ (Prodigy checkpoint)

```bash
./tools/resume_ko_prodigy.sh
```

**ì¡°ê±´**:
- latest.pthê°€ Prodigy optimizerë¡œ ì €ì¥ëœ ê²½ìš°ë§Œ ì‚¬ìš©
- AdamW â†’ Prodigy ì „í™˜ ì‹œ ë°˜ë“œì‹œ ìƒˆë¡œ ì‹œì‘

---

## ğŸ“ ì½”ë“œ ë³€ê²½ì‚¬í•­

### 1. trainers/train_gpt_v2.py

**ì£¼ìš” ë³€ê²½**:

1. **Prodigy optimizer ì¶”ê°€** (Line 36):
```python
from prodigyopt import Prodigy
```

2. **Optimizer ì„ íƒ ë¡œì§** (Line 943-952):
```python
if args.optimizer == "prodigy":
    optimizer = Prodigy(
        model.parameters(),
        lr=1.0,
        weight_decay=args.weight_decay,
        d_coef=1.0,
        use_bias_correction=False,
        safeguard_warmup=False,
    )
else:
    optimizer = AdamW(...)
```

3. **Smart Resume Logic** (Line 965-972):
```python
ckpt_optimizer_type = checkpoint.get("optimizer_type", "adamw")
if args.optimizer == ckpt_optimizer_type:
    optimizer.load_state_dict(checkpoint["optimizer"])
else:
    print(f"[Info] Skipping optimizer state (incompatible)")
```

4. **Checkpointì— optimizer_type ì €ì¥** (Line 1287-1297):
```python
torch.save({
    "model": model.state_dict(),
    "optimizer": optimizer.state_dict(),
    "optimizer_type": args.optimizer,  # ì¶”ê°€!
    ...
}, output_dir / "latest.pth")
```

5. **GPU ìµœì í™”** (Line 755-766):
```python
# Flash Attention 2
torch.backends.cuda.enable_flash_sdp(True)
torch.backends.cuda.enable_mem_efficient_sdp(True)

# cuDNN + Matmul
torch.backends.cudnn.benchmark = True
torch.set_float32_matmul_precision("high")
```

6. **torch.compile** (Line 837-840):
```python
model = torch.compile(model, mode="reduce-overhead")
```

7. **BFloat16 AMP** (Line 1047):
```python
with torch.cuda.amp.autocast(enabled=use_amp, dtype=torch.bfloat16 if use_amp else torch.float32):
    outputs = model(...)
```

8. **DataLoader ìµœì í™”** (Line 916-940):
```python
dataloader_kwargs = {
    "persistent_workers": args.num_workers > 0,
    "prefetch_factor": 2,
    "multiprocessing_context": "fork",
}
```

### 2. tools/ko_step4_train_gpt.sh

**ë³€ê²½**:
```bash
OPTIMIZER_FLAG="${OPTIMIZER:-adamw}"

CMD+=(
  --optimizer "${OPTIMIZER_FLAG}"
)
```

### 3. ìƒˆë¡œìš´ ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼

**tools/train_ko_prodigy.sh**: Fresh start
**tools/resume_ko_prodigy.sh**: Resume from Prodigy checkpoint

---

## ğŸ” ê²€ì¦ ë°©ë²•

### 1. í•™ìŠµ ì†ë„ í™•ì¸

**LOG_INTERVAL=100**ìœ¼ë¡œ ì„¤ì •í–ˆìœ¼ë¯€ë¡œ 100 stepë§ˆë‹¤ ì¶œë ¥:

```
[Step 100] Loss: 2.34, Text Loss: 1.23, Mel Loss: 1.11, LR: 0.00012, Time: 45.2s
```

**ì´ˆê¸° AdamW vs Prodigy ë¹„êµ**:
- AdamW: ~0.52s/step (Step 16,000 ê¸°ì¤€)
- Prodigy: **~0.20s/step ì˜ˆìƒ** (2.6Ã— ë¹ ë¦„)

### 2. GPU í™œìš© í™•ì¸

```bash
nvidia-smi --query-gpu=index,memory.used,utilization.gpu --format=csv,noheader,nounits
```

**ì •ìƒ ìƒíƒœ**:
- Memory: 20-22 GB
- Utilization: 95-100%

### 3. Best Checkpoint ëª¨ë‹ˆí„°ë§

```bash
tail -f /tmp/best_ckpt_monitor.log
```

**Best checkpoint ìë™ ì €ì¥**:
- best_text_model.pth: ê°€ì¥ ë‚®ì€ text loss
- best_mel_model.pth: ê°€ì¥ ë‚®ì€ mel loss

---

## âš ï¸ ì£¼ì˜ì‚¬í•­

### 1. AdamW â†’ Prodigy ì „í™˜

**ì ˆëŒ€ ê¸ˆì§€**:
```bash
# âŒ AdamW checkpointì—ì„œ Prodigy resume
RESUME=/path/to/adamw_latest.pth ./tools/resume_ko_prodigy.sh
```

**ì˜¬ë°”ë¥¸ ë°©ë²•**:
```bash
# âœ… Step 0ë¶€í„° Prodigyë¡œ ìƒˆë¡œ ì‹œì‘
./tools/train_ko_prodigy.sh
```

**ì´ìœ **: AdamWì™€ Prodigyì˜ optimizer state êµ¬ì¡°ê°€ ë‹¤ë¦„
- AdamW: `exp_avg`, `exp_avg_sq`
- Prodigy: `exp_avg`, `exp_avg_sq`, `d`, `s`, `k`

### 2. torch.compile ì²« ì‹¤í–‰

**ì²« ë²ˆì§¸ stepì€ 10-20ë¶„ ì†Œìš” ê°€ëŠ¥**:
- JIT compilation ì§„í–‰ ì¤‘
- GPU 100% ì‚¬ìš© ì¤‘ì´ë©´ ì •ìƒ
- ì´í›„ ëª¨ë“  stepì€ ë¹ ë¥´ê²Œ ì‹¤í–‰

### 3. BFloat16 ì§€ì› í™•ì¸

**RTX 30xx/40xxë§Œ ì§€ì›**:
```python
# ìë™ìœ¼ë¡œ FP32ë¡œ fallbackë˜ë¯€ë¡œ ì•ˆì „
torch.cuda.is_bf16_supported()  # Trueë©´ BFloat16 ì‚¬ìš©
```

---

## ğŸ”„ Prodigy vs Optuna

| í•­ëª© | Prodigy | Optuna |
|-----|---------|--------|
| **ëª©ì ** | Optimizer (AdamW ëŒ€ì²´) | Hyperparameter tuner |
| **ì‚¬ìš© ì‹œì ** | í•™ìŠµ ì¤‘ (ë§¤ step) | í•™ìŠµ ì „ (multiple runs) |
| **ì¡°ì • ëŒ€ìƒ** | Learning rate (ìë™) | Batch size, d_coef, etc. |
| **ì‹¤í–‰ íšŸìˆ˜** | 1íšŒ í•™ìŠµ | NíšŒ í•™ìŠµ (trial) |
| **ì ìš© ì‹œê¸°** | âœ… ì§€ê¸ˆ ë°”ë¡œ | Phase 1 ì™„ë£Œ í›„ (ì„ íƒ) |

**ê²°ë¡ **: Prodigyë¥¼ ë¨¼ì € ì‚¬ìš©í•˜ê³ , í•„ìš”í•˜ë©´ Optunaë¡œ Prodigy íŒŒë¼ë¯¸í„° íŠœë‹

---

## ğŸ“š ì°¸ê³  ìë£Œ

### ë…¼ë¬¸
- **Prodigy**: "Prodigy: An Expeditiously Adaptive Parameter-Free Learner" (ICLR 2025)
  - https://arxiv.org/abs/2306.06101
- **Flash Attention 2**: "FlashAttention-2: Faster Attention with Better Parallelism" (2023)
  - https://arxiv.org/abs/2307.08691

### ë¼ì´ë¸ŒëŸ¬ë¦¬
- **prodigyopt**: https://github.com/konstmish/prodigy
- **torch.compile**: https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html

---

## ğŸ“Š ì‹¤í—˜ ì¶”ì 

### Aim (Experiment Tracker)

**ì‹¤í–‰**:
```bash
cd /mnt/sdc1/ws/workspace/monorepo/external/index-tts
aim up --repo .aim
```

**URL**: http://localhost:43800

**ì¶”ì  ë©”íŠ¸ë¦­**:
- Train loss (text, mel, total)
- Validation loss
- Learning rate (Prodigy auto-adjusted)
- Gradient norm

---

## âœ… ì²´í¬ë¦¬ìŠ¤íŠ¸

í•™ìŠµ ì‹œì‘ ì „:
- [ ] Prodigy optimizer ì„¤ì¹˜ (`uv pip install prodigyopt`)
- [ ] PyTorch 2.0+ í™•ì¸
- [ ] RTX 30xx/40xx GPU í™•ì¸ (BFloat16 ì§€ì›)
- [ ] ì¶©ë¶„í•œ ë””ìŠ¤í¬ ê³µê°„ (checkpoint ì €ì¥ìš©)

í•™ìŠµ ì¤‘:
- [ ] GPU 100% í™œìš© í™•ì¸
- [ ] ì²« step (10-20ë¶„) ëŒ€ê¸°
- [ ] LOG_INTERVALë§ˆë‹¤ loss ê°ì†Œ í™•ì¸
- [ ] Best checkpoint ìë™ ì €ì¥ í™•ì¸

---

## ğŸ‰ ê²°ë¡ 

**ì´ 7ê°€ì§€ ìµœì í™” ì ìš©**:
1. Prodigy Optimizer (ìë™ LR)
2. torch.compile (JIT)
3. Flash Attention 2
4. BFloat16 AMP
5. cuDNN Benchmark
6. Matmul Precision
7. DataLoader Optimization

**ì˜ˆìƒ íš¨ê³¼**:
- **2.4Ã— ì†ë„ í–¥ìƒ** (86h â†’ 36h)
- **ìë™ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°ì •**
- **ì•ˆì •ì ì¸ í•™ìŠµ**

**ìµœì¢… ëª…ë ¹**:
```bash
cd /mnt/sdc1/ws/workspace/monorepo/external/index-tts
./tools/train_ko_prodigy.sh
```

í•™ìŠµì„ ì‹œì‘í•˜ê³  ì²« 100 step ë¡œê·¸ë¥¼ í™•ì¸í•˜ì„¸ìš”! ğŸš€
