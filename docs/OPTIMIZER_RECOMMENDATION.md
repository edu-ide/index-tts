# IndexTTS-2를 위한 Optimizer 추천

추가 리서치 결과(Schedule-Free AdamW, SOAP, AdEMAMix)를 바탕으로, 특히 **Audio/TTS 도메인**과 **작은 배치 사이즈(Small Batch)** 제약 조건을 고려한 추천입니다.

## 📊 요약

| 특징 | Schedule-Free AdamW | SOAP | AdEMAMix |
| :--- | :--- | :--- | :--- |
| **주요 장점** | LR 스케줄 불필요, 빠른 수렴 | 높은 효율성 (적은 스텝 수) | **노이즈가 심한 그라디언트에 강함 (Small Batch)** |
| **배치 사이즈** | 무관 (AdamW와 유사) | **Large Batch 필수** (현재 상황과 불일치) | **Small Batch 최적화** (현재 상황과 일치) |
| **Audio/TTS 검증** | 긍정적 (ASR 태스크 등) | 없음 | 없음 (이론적으로는 적합) |
| **구현 위험도** | 낮음 | 높음 | 중간 |
| **예상 속도/효율** | AdamW 대비 ~1.0-1.5배 | 오버헤드로 느려질 수 있음 | AdamW 대비 **~2배 효율** |

---

## 🚫 비추천: SOAP
**판단**: **이번 태스크에는 부적합합니다.**

*   **이유**: SOAP은 **대형 배치 사이즈**(예: 4k+)에서 효과를 발휘하도록 설계되었습니다.
*   **충돌**: 현재 배치 사이즈는 **8**입니다. SOAP과 같은 2차(Second-order) optimizer는 곡률(curvature)을 추정해야 하는데, 배치가 작으면 이 추정이 매우 부정확하고 불안정해집니다.
*   **위험**: 불안정성, 발산, 또는 계산 오버헤드가 이득보다 클 확률이 높습니다.

---

## 🥈 안전한 선택: Schedule-Free AdamW
**판단**: **좋은 차선책 / 보수적인 선택.**

*   **이유**: 학습 루프를 단순화(스케줄러 튜닝 불필요)하고, 일반적으로 AdamW보다 낫거나 비슷합니다.
*   **장점**: "설정하고 잊기(Set and forget)"가 가능합니다. LR 스케줄 튜닝에 지쳤다면 좋은 선택입니다.
*   **단점**: AdEMAMix만큼 "작은 배치의 노이즈 문제"를 근본적으로 해결하지는 못합니다.

---

## 🥇 강력 추천: AdEMAMix
**판단**: **현재 제약 조건(Small Batch)에 가장 적합합니다.**

*   **왜?**: AdEMAMix는 **그라디언트 노이즈가 심한 상황**을 처리하도록 특별히 설계되었습니다. 배치 사이즈 8인 현재 상황이 딱 그렇습니다.
*   **원리**: 두 개의 EMA(빠른 것, 느린 것)를 사용하여 노이즈 속에서도 진정한 그라디언트 방향을 더 잘 추정합니다.
*   **효율성**: 연구에 따르면 작은 배치 영역에서 AdamW 대비 **~50% 적은 스텝**으로 동일한 학습 손실에 도달할 수 있습니다.
*   **시너지**: 단순히 "더 빠른" 것이 아니라, 현재 설정의 약점(작은 배치 사이즈)을 직접적으로 보완합니다.

---

## 🚀 제안하는 실행 계획

1.  **AdEMAMix 구현**:
    *   구현이 비교적 간단합니다 (AdamW 로직의 래퍼 형태).
    *   현재 가장 큰 병목(스텝당 샘플 효율)을 타겟팅합니다.

2.  **플랜 B (Fallback)**:
    *   만약 AdEMAMix가 불안정하다면(TTS 특성상 가능성 있음), **Schedule-Free AdamW**로 전환합니다.

3.  **다음 단계**:
    *   제가 `AdEMAMix` optimizer 코드를 생성해 드릴 수 있습니다.
    *   `train_gpt_v2.py`에 이를 통합하는 작업을 진행하겠습니다.
